#!/usr/bin/env python3
"""
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°åˆ†æãƒ»å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
UIã¨å‡¦ç†ã«å½±éŸ¿ã—ãªã„ã‚³ãƒ¼ãƒ‰å“è³ªå‘ä¸Šã®ãŸã‚ã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°
"""

import os
import re
import json
import ast
import argparse
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict, Counter

@dataclass
class CodeIssue:
    """ã‚³ãƒ¼ãƒ‰ã®å•é¡Œã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    file_path: str
    line_number: int
    issue_type: str
    description: str
    severity: str  # 'high', 'medium', 'low'
    suggestion: str

@dataclass
class RefactoringSuggestion:
    """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ææ¡ˆã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    category: str
    priority: int
    description: str
    files_affected: List[str]
    estimated_effort: str
    benefits: List[str]

class ProjectAnalyzer:
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.src_dir = self.project_root / "src"
        self.issues: List[CodeIssue] = []
        self.suggestions: List[RefactoringSuggestion] = []
        
    def analyze_all(self) -> Dict:
        """å…¨ä½“åˆ†æã‚’å®Ÿè¡Œ"""
        print("ğŸ” ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“åˆ†æã‚’é–‹å§‹...")
        
        results = {
            "file_stats": self._analyze_file_statistics(),
            "duplicate_code": self._find_duplicate_code(),
            "component_issues": self._analyze_components(),
            "type_issues": self._analyze_types(),
            "import_issues": self._analyze_imports(),
            "utility_functions": self._analyze_utility_functions(),
            "complexity_metrics": self._analyze_complexity(),
            "suggestions": self._generate_suggestions()
        }
        
        return results
    
    def _analyze_file_statistics(self) -> Dict:
        """ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆåˆ†æ"""
        stats = {
            "total_files": 0,
            "total_lines": 0,
            "large_files": [],
            "file_distribution": defaultdict(int)
        }
        
        for file_path in self._get_source_files():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = len(f.readlines())
                
                stats["total_files"] += 1
                stats["total_lines"] += lines
                
                # å¤§ãã™ãã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œå‡º
                if lines > 300:
                    stats["large_files"].append({
                        "path": str(file_path.relative_to(self.project_root)),
                        "lines": lines,
                        "severity": "high" if lines > 500 else "medium"
                    })
                
                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåˆ¥çµ±è¨ˆ
                relative_path = file_path.relative_to(self.src_dir)
                directory = str(relative_path.parent) if relative_path.parent != Path('.') else 'root'
                stats["file_distribution"][directory] += 1
                
            except Exception as e:
                print(f"è­¦å‘Š: {file_path} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        
        return stats
    
    def _find_duplicate_code(self) -> Dict:
        """é‡è¤‡ã‚³ãƒ¼ãƒ‰æ¤œå‡º"""
        duplicates = {
            "function_names": Counter(),
            "import_statements": Counter(),
            "similar_patterns": [],
            "duplicate_interfaces": []
        }
        
        for file_path in self._get_source_files():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # é–¢æ•°åã®é‡è¤‡
                function_matches = re.findall(r'(?:function\s+|const\s+)(\w+)', content)
                for func_name in function_matches:
                    duplicates["function_names"][func_name] += 1
                
                # ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–‡ã®é‡è¤‡
                import_matches = re.findall(r'import.*from\s+["\']([^"\']+)["\']', content)
                for import_path in import_matches:
                    duplicates["import_statements"][import_path] += 1
                
                # ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®šç¾©ã®é‡è¤‡
                interface_matches = re.findall(r'interface\s+(\w+)', content)
                for interface_name in interface_matches:
                    if interface_name in duplicates["duplicate_interfaces"]:
                        continue
                    # ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§åŒã˜ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹åãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    count = 0
                    for other_file in self._get_source_files():
                        if other_file != file_path:
                            try:
                                with open(other_file, 'r', encoding='utf-8') as f:
                                    other_content = f.read()
                                if f'interface {interface_name}' in other_content:
                                    count += 1
                            except:
                                pass
                    if count > 0:
                        duplicates["duplicate_interfaces"].append({
                            "name": interface_name,
                            "occurrences": count + 1
                        })
                
            except Exception as e:
                print(f"è­¦å‘Š: {file_path} ã®é‡è¤‡ã‚³ãƒ¼ãƒ‰åˆ†æã«å¤±æ•—: {e}")
        
        # é »ç¹ã«ä½¿ç”¨ã•ã‚Œã‚‹é …ç›®ã®ã¿æŠ½å‡º
        duplicates["frequent_functions"] = {k: v for k, v in duplicates["function_names"].items() if v > 2}
        duplicates["common_imports"] = {k: v for k, v in duplicates["import_statements"].items() if v > 5}
        
        return duplicates
    
    def _analyze_components(self) -> Dict:
        """React ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ†æ"""
        component_issues = {
            "large_components": [],
            "complex_components": [],
            "stateful_components": [],
            "prop_issues": []
        }
        
        components_dir = self.src_dir / "components"
        if not components_dir.exists():
            return component_issues
        
        for file_path in components_dir.rglob("*.tsx"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                lines = len(content.splitlines())
                
                # å¤§ãã™ãã‚‹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
                if lines > 200:
                    component_issues["large_components"].append({
                        "path": str(file_path.relative_to(self.project_root)),
                        "lines": lines,
                        "suggestion": "å°ã•ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«åˆ†å‰²ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨"
                    })
                
                # è¤‡é›‘ãªçŠ¶æ…‹ç®¡ç†
                useState_count = len(re.findall(r'useState', content))
                useEffect_count = len(re.findall(r'useEffect', content))
                
                if useState_count > 5:
                    component_issues["stateful_components"].append({
                        "path": str(file_path.relative_to(self.project_root)),
                        "useState_count": useState_count,
                        "useEffect_count": useEffect_count,
                        "suggestion": "ã‚«ã‚¹ã‚¿ãƒ ãƒ•ãƒƒã‚¯ã¾ãŸã¯çŠ¶æ…‹ç®¡ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ä½¿ç”¨ã‚’æ¤œè¨"
                    })
                
                # ãƒ—ãƒ­ãƒƒãƒ—ã®å‹ãƒã‚§ãƒƒã‚¯
                any_props = len(re.findall(r'props:\s*any', content))
                if any_props > 0:
                    component_issues["prop_issues"].append({
                        "path": str(file_path.relative_to(self.project_root)),
                        "any_usage": any_props,
                        "suggestion": "é©åˆ‡ãªå‹å®šç¾©ã‚’ä½œæˆ"
                    })
                
            except Exception as e:
                print(f"è­¦å‘Š: {file_path} ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ†æã«å¤±æ•—: {e}")
        
        return component_issues
    
    def _analyze_types(self) -> Dict:
        """å‹å®šç¾©åˆ†æ"""
        type_issues = {
            "any_usage": [],
            "inline_interfaces": [],
            "missing_types": [],
            "type_duplicates": []
        }
        
        for file_path in self._get_source_files():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # anyå‹ã®ä½¿ç”¨
                any_matches = re.finditer(r':\s*any\b|<any>', content)
                for match in any_matches:
                    line_num = content[:match.start()].count('\n') + 1
                    type_issues["any_usage"].append({
                        "path": str(file_path.relative_to(self.project_root)),
                        "line": line_num,
                        "context": self._get_line_context(content, line_num)
                    })
                
                # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
                if 'types/' not in str(file_path):
                    interface_matches = re.finditer(r'interface\s+\w+\s*{', content)
                    for match in interface_matches:
                        line_num = content[:match.start()].count('\n') + 1
                        type_issues["inline_interfaces"].append({
                            "path": str(file_path.relative_to(self.project_root)),
                            "line": line_num,
                            "interface": match.group(),
                            "suggestion": "types/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•ã‚’æ¤œè¨"
                        })
                
            except Exception as e:
                print(f"è­¦å‘Š: {file_path} ã®å‹åˆ†æã«å¤±æ•—: {e}")
        
        return type_issues
    
    def _analyze_imports(self) -> Dict:
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆåˆ†æ"""
        import_issues = {
            "unused_imports": [],
            "redundant_imports": [],
            "long_import_paths": [],
            "missing_index_files": []
        }
        
        for file_path in self._get_source_files():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–‡ã®æŠ½å‡º
                import_matches = re.findall(r'import\s+{([^}]+)}\s+from\s+["\']([^"\']+)["\']', content)
                
                for imports, module_path in import_matches:
                    # é•·ã™ãã‚‹ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‘ã‚¹
                    if len(module_path.split('/')) > 4:
                        import_issues["long_import_paths"].append({
                            "path": str(file_path.relative_to(self.project_root)),
                            "import_path": module_path,
                            "suggestion": "index.tsãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ãŸçŸ­ç¸®ã‚’æ¤œè¨"
                        })
                    
                    # æœªä½¿ç”¨ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                    imported_names = [name.strip() for name in imports.split(',')]
                    for name in imported_names:
                        name = name.strip()
                        if name and not re.search(rf'\b{re.escape(name)}\b', content.replace(f'import {{{imports}}}', '')):
                            import_issues["unused_imports"].append({
                                "path": str(file_path.relative_to(self.project_root)),
                                "unused_import": name,
                                "from_module": module_path
                            })
                
            except Exception as e:
                print(f"è­¦å‘Š: {file_path} ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆåˆ†æã«å¤±æ•—: {e}")
        
        return import_issues
    
    def _analyze_utility_functions(self) -> Dict:
        """ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°åˆ†æ"""
        utils_analysis = {
            "inline_functions": [],
            "extractable_functions": [],
            "utility_candidates": []
        }
        
        # lib/utils/ ä»¥å¤–ã®å ´æ‰€ã§å®šç¾©ã•ã‚Œã¦ã„ã‚‹é–¢æ•°ã‚’æ¢ã™
        for file_path in self._get_source_files():
            if 'lib/utils' in str(file_path) or 'utils/' in str(file_path):
                continue
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³é–¢æ•°ã®æ¤œå‡º
                inline_functions = re.findall(r'const\s+(\w+)\s*=\s*\([^)]*\)\s*=>', content)
                if len(inline_functions) > 3:
                    utils_analysis["inline_functions"].append({
                        "path": str(file_path.relative_to(self.project_root)),
                        "function_count": len(inline_functions),
                        "functions": inline_functions,
                        "suggestion": "ä¸€éƒ¨ã®é–¢æ•°ã‚’ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã¨ã—ã¦æŠ½å‡ºã‚’æ¤œè¨"
                    })
                
            except Exception as e:
                print(f"è­¦å‘Š: {file_path} ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£åˆ†æã«å¤±æ•—: {e}")
        
        return utils_analysis
    
    def _analyze_complexity(self) -> Dict:
        """ã‚³ãƒ¼ãƒ‰ã®è¤‡é›‘æ€§åˆ†æ"""
        complexity = {
            "cyclomatic_complexity": [],
            "nested_conditions": [],
            "long_functions": []
        }
        
        for file_path in self._get_source_files():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æ·±ã„ãƒã‚¹ãƒˆã®æ¤œå‡º
                lines = content.splitlines()
                for i, line in enumerate(lines):
                    indent_level = len(line) - len(line.lstrip())
                    if indent_level > 16:  # 4ã‚¹ãƒšãƒ¼ã‚¹ Ã— 4ãƒ¬ãƒ™ãƒ«ä»¥ä¸Š
                        complexity["nested_conditions"].append({
                            "path": str(file_path.relative_to(self.project_root)),
                            "line": i + 1,
                            "indent_level": indent_level // 2,
                            "suggestion": "ãƒã‚¹ãƒˆã‚’æµ…ãã™ã‚‹ãŸã‚ã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã‚’æ¤œè¨"
                        })
                
            except Exception as e:
                print(f"è­¦å‘Š: {file_path} ã®è¤‡é›‘æ€§åˆ†æã«å¤±æ•—: {e}")
        
        return complexity
    
    def _generate_suggestions(self) -> List[RefactoringSuggestion]:
        """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ææ¡ˆç”Ÿæˆ"""
        suggestions = []
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ†å‰²ã®ææ¡ˆ
        suggestions.append(RefactoringSuggestion(
            category="ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ†å‰²",
            priority=1,
            description="å¤§ãã™ãã‚‹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’å°ã•ãªå†åˆ©ç”¨å¯èƒ½ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«åˆ†å‰²",
            files_affected=["src/components/**/*.tsx"],
            estimated_effort="ä¸­ç¨‹åº¦",
            benefits=[
                "ã‚³ãƒ¼ãƒ‰ã®å¯èª­æ€§å‘ä¸Š",
                "ãƒ†ã‚¹ãƒˆã®æ›¸ãã‚„ã™ã•",
                "å†åˆ©ç”¨æ€§ã®å‘ä¸Š"
            ]
        ))
        
        # å‹å®‰å…¨æ€§ã®å‘ä¸Š
        suggestions.append(RefactoringSuggestion(
            category="å‹å®‰å…¨æ€§å‘ä¸Š",
            priority=2,
            description="anyå‹ã®ä½¿ç”¨ã‚’é¿ã‘ã€é©åˆ‡ãªå‹å®šç¾©ã‚’ä½œæˆ",
            files_affected=["src/**/*.ts", "src/**/*.tsx"],
            estimated_effort="å°ç¨‹åº¦",
            benefits=[
                "å‹å®‰å…¨æ€§ã®å‘ä¸Š",
                "IDEã‚µãƒãƒ¼ãƒˆã®æ”¹å–„",
                "ãƒã‚°ã®æ—©æœŸç™ºè¦‹"
            ]
        ))
        
        # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã®å…±é€šåŒ–
        suggestions.append(RefactoringSuggestion(
            category="ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£å…±é€šåŒ–",
            priority=3,
            description="é‡è¤‡ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã‚’å…±é€šåŒ–",
            files_affected=["src/lib/utils/**/*.ts"],
            estimated_effort="å°ç¨‹åº¦",
            benefits=[
                "ã‚³ãƒ¼ãƒ‰ã®é‡è¤‡å‰Šæ¸›",
                "ä¿å®ˆæ€§ã®å‘ä¸Š",
                "ä¸€è²«æ€§ã®ç¢ºä¿"
            ]
        ))
        
        return suggestions
    
    def _get_source_files(self) -> List[Path]:
        """ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—"""
        extensions = {'.ts', '.tsx', '.js', '.jsx'}
        files = []
        
        for ext in extensions:
            files.extend(self.src_dir.rglob(f"*{ext}"))
        
        # node_modulesç­‰ã‚’é™¤å¤–
        exclude_patterns = {'node_modules', '.next', 'dist', 'build'}
        return [f for f in files if not any(pattern in str(f) for pattern in exclude_patterns)]
    
    def _get_line_context(self, content: str, line_num: int, context_lines: int = 2) -> str:
        """æŒ‡å®šè¡Œã®å‰å¾Œã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
        lines = content.splitlines()
        start = max(0, line_num - context_lines - 1)
        end = min(len(lines), line_num + context_lines)
        return '\n'.join(lines[start:end])

class RefactoringExecutor:
    """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.analyzer = ProjectAnalyzer(project_root)
    
    def execute_refactoring(self, categories: List[str], dry_run: bool = True):
        """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
        print(f"ğŸ”§ ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ (dry_run={dry_run})")
        
        if 'components' in categories:
            self._refactor_components(dry_run)
        
        if 'types' in categories:
            self._refactor_types(dry_run)
        
        if 'utils' in categories:
            self._refactor_utils(dry_run)
        
        if 'imports' in categories:
            self._refactor_imports(dry_run)
    
    def _refactor_components(self, dry_run: bool):
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°"""
        print("ğŸ§© ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
        # å®Ÿéš›ã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ã¯ã“ã“ã«å®Ÿè£…
        # Claude Codeã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã®é€£æºãŒå¿…è¦
        pass
    
    def _refactor_types(self, dry_run: bool):
        """å‹å®šç¾©ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°"""
        print("ğŸ“ å‹å®šç¾©ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
        pass
    
    def _refactor_utils(self, dry_run: bool):
        """ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°"""
        print("ğŸ› ï¸ ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
        pass
    
    def _refactor_imports(self, dry_run: bool):
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°"""
        print("ğŸ“¦ ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
        pass

def generate_report(analysis_results: Dict, output_path: str):
    """åˆ†æçµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    
    report_content = f"""# ğŸ”§ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

ç”Ÿæˆæ—¥æ™‚: {os.popen('date').read().strip()}

## ğŸ“Š ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆçµ±è¨ˆ

### ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ
- ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {analysis_results['file_stats']['total_files']}
- ç·ã‚³ãƒ¼ãƒ‰è¡Œæ•°: {analysis_results['file_stats']['total_lines']:,}
- å¤§ãã™ãã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(analysis_results['file_stats']['large_files'])}

### ğŸš¨ æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ

#### å¤§ãã™ãã‚‹ãƒ•ã‚¡ã‚¤ãƒ« (300è¡Œä»¥ä¸Š)
"""
    
    for large_file in analysis_results['file_stats']['large_files']:
        report_content += f"- `{large_file['path']}`: {large_file['lines']}è¡Œ ({large_file['severity']})\n"
    
    report_content += f"""
#### ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å•é¡Œ
- å¤§ãã™ãã‚‹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ: {len(analysis_results['component_issues']['large_components'])}å€‹
- çŠ¶æ…‹ãŒå¤šã™ãã‚‹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ: {len(analysis_results['component_issues']['stateful_components'])}å€‹

#### å‹ã®å•é¡Œ
- anyå‹ã®ä½¿ç”¨ç®‡æ‰€: {len(analysis_results['type_issues']['any_usage'])}ç®‡æ‰€
- ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ interface: {len(analysis_results['type_issues']['inline_interfaces'])}å€‹

#### ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®å•é¡Œ
- æœªä½¿ç”¨ã®å¯èƒ½æ€§ãŒã‚ã‚‹ã‚¤ãƒ³ãƒãƒ¼ãƒˆ: {len(analysis_results['import_issues']['unused_imports'])}å€‹
- é•·ã™ãã‚‹ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‘ã‚¹: {len(analysis_results['import_issues']['long_import_paths'])}å€‹

## ğŸ¯ æ¨å¥¨ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°

"""
    
    for suggestion in analysis_results['suggestions']:
        report_content += f"""### {suggestion.category}
- **å„ªå…ˆåº¦**: {suggestion.priority}
- **èª¬æ˜**: {suggestion.description}
- **æ¨å®šå·¥æ•°**: {suggestion.estimated_effort}
- **åŠ¹æœ**: {', '.join(suggestion.benefits)}

"""
    
    report_content += """
## ğŸ“‹ å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰

```bash
# å…¨ä½“åˆ†æ
/refactor --analyze-only

# ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°
/refactor --components

# å‹å®šç¾©æ•´ç†
/refactor --types

# å…¨ä½“ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°
/refactor --all
```

---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ*
"""
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ: {output_path}")

def main():
    parser = argparse.ArgumentParser(description='ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°åˆ†æãƒ»å®Ÿè¡Œ')
    parser.add_argument('--project-root', default='.', help='ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--analyze-only', action='store_true', help='åˆ†æã®ã¿å®Ÿè¡Œ')
    parser.add_argument('--categories', nargs='+', 
                       choices=['components', 'types', 'utils', 'imports'], 
                       default=['components', 'types', 'utils', 'imports'],
                       help='ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã‚«ãƒ†ã‚´ãƒª')
    parser.add_argument('--dry-run', action='store_true', help='å®Ÿéš›ã®å¤‰æ›´ã¯è¡Œã‚ãªã„')
    parser.add_argument('--output', default='refactor_report.md', help='ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«')
    
    args = parser.parse_args()
    
    # åˆ†æå®Ÿè¡Œ
    analyzer = ProjectAnalyzer(args.project_root)
    results = analyzer.analyze_all()
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    generate_report(results, args.output)
    
    # ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
    if not args.analyze_only:
        executor = RefactoringExecutor(args.project_root)
        executor.execute_refactoring(args.categories, args.dry_run)

if __name__ == "__main__":
    main()